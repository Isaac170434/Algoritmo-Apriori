{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Laboratorio de Preprocesamiento.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Mineria de Datos**\n",
        "# **Laboratorio de Preprocesamiento**\n",
        "-----------------------------------\n",
        "### **Nombres:** Antony Isaac Huaman Hermoza\n",
        "### **Código:** 170434\n",
        "-----------------------------------"
      ],
      "metadata": {
        "id": "xGsj7XKrCqGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Instalamos PySpark**"
      ],
      "metadata": {
        "id": "IQXf0XMRChP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark==3.0.1 py4j==0.10.9"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paTsqIDFP78p",
        "outputId": "631fc1b3-4f51-4b62-979d-d7c426fb22fa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark==3.0.1\n",
            "  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2 MB 30 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 53.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612246 sha256=50a7daacc6d864a74ddca0d382819adaab044f2c1f844640ff67536692e3508b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/34/fa/b37b5cef503fc5148b478b2495043ba61b079120b7ff379f9b\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importamos librerias**"
      ],
      "metadata": {
        "id": "1px1rn5oCb4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##############################\n",
        "#Para PySpark en general\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext.getOrCreate()\n",
        "##############################\n",
        "#Complementarias\n",
        "import pyspark.sql.functions as F\n",
        "from functools import reduce\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql import SparkSession\n",
        "import math\n",
        "##############################\n",
        "#Liberias utilizadas para el escalonamiento\n",
        "import IPython\n",
        "from pyspark import SparkConf \n",
        "from pyspark.context import SparkContext \n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import DoubleType\n",
        "##############################\n",
        "#Liberias utilizadas para normalización\n",
        "import math\n",
        "from pyspark.sql.functions import lit\n",
        "from pyspark.sql.types import FloatType\n",
        "###############################\n",
        "#Liberias utilizadas para binarización\n",
        "import pyspark.sql.functions as F\n",
        "from functools import reduce\n",
        "###############################\n",
        "#Liberias utilizadas para Distancia de Minkowski\n",
        "import numpy as np\n",
        "###############################\n",
        "#Liberias utilizadas para Distancia Jaccard utiliza las anteriores"
      ],
      "metadata": {
        "id": "wLeMy8K7P9HN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Escalonamiento**"
      ],
      "metadata": {
        "id": "-0BuRDQNP-AR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "metadata": {
        "id": "E3tqsV8LQDUf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creamos datos\n",
        "df = spark.createDataFrame([ (1, 'Messi',12560,45,\"True\"),\n",
        "                             (2, 'James',42560,90,\"False\"),\n",
        "                             (3, 'Oliver',31285,81,\"True\"),\n",
        "                             (4, 'Isaac',25285,50,\"True\"),\n",
        "                             (5, 'Irving',32285,55,\"False\"),\n",
        "                             (6, 'Franco',18085,61,\"True\"),\n",
        "                             (7, 'Leonel',52185,70,\"False\"),\n",
        "                           ], [\"id_usuario\", \"Nombre\",\"Ingresos\",\"Por_dia\",\"Morocidad\"])\n",
        "\n",
        "#Mostrar datos iniciales\n",
        "print(\"Data :\")\n",
        "df.show(7)\n",
        "\n",
        "#UDF para convertir el tipo de columna de vector a tipo doble\n",
        "unlist = udf(lambda x: round(float(list(x)[0]),3), DoubleType())\n",
        "\n",
        "#Iterando sobre columnas para el escalonamiento\n",
        "for i in [\"Ingresos\",\"Por_dia\"]:\n",
        "    #Transformación con VectorAssembler (conversión de columna a tipo vector)\n",
        "    assembler = VectorAssembler(inputCols=[i],outputCol=i+\"_Vect\")\n",
        "\n",
        "    #Transformación MinMaxScaler\n",
        "    scaler = MinMaxScaler(inputCol=i+\"_Vect\", outputCol=i+\"_Escalonado\")\n",
        "\n",
        "    #Canalización de VectorAssembler y MinMaxScaler\n",
        "    pipeline = Pipeline(stages=[assembler, scaler])\n",
        "\n",
        "    #Ajuste del pipeline en el marco de datos\n",
        "    df = pipeline.fit(df).transform(df).withColumn(i+\"_Escalonado\", unlist(i+\"_Escalonado\")).drop(i+\"_Vect\")\n",
        "\n",
        "print(\"Data + Escalonamiento :\")\n",
        "df.show(7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k01RCxQJQF2l",
        "outputId": "adcd1786-4a5f-43aa-d73f-850d63bfc1d4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data :\n",
            "+----------+------+--------+-------+---------+\n",
            "|id_usuario|Nombre|Ingresos|Por_dia|Morocidad|\n",
            "+----------+------+--------+-------+---------+\n",
            "|         1| Messi|   12560|     45|     True|\n",
            "|         2| James|   42560|     90|    False|\n",
            "|         3|Oliver|   31285|     81|     True|\n",
            "|         4| Isaac|   25285|     50|     True|\n",
            "|         5|Irving|   32285|     55|    False|\n",
            "|         6|Franco|   18085|     61|     True|\n",
            "|         7|Leonel|   52185|     70|    False|\n",
            "+----------+------+--------+-------+---------+\n",
            "\n",
            "Data + Escalonamiento :\n",
            "+----------+------+--------+-------+---------+-------------------+------------------+\n",
            "|id_usuario|Nombre|Ingresos|Por_dia|Morocidad|Ingresos_Escalonado|Por_dia_Escalonado|\n",
            "+----------+------+--------+-------+---------+-------------------+------------------+\n",
            "|         1| Messi|   12560|     45|     True|                0.0|               0.0|\n",
            "|         2| James|   42560|     90|    False|              0.757|               1.0|\n",
            "|         3|Oliver|   31285|     81|     True|              0.473|               0.8|\n",
            "|         4| Isaac|   25285|     50|     True|              0.321|             0.111|\n",
            "|         5|Irving|   32285|     55|    False|              0.498|             0.222|\n",
            "|         6|Franco|   18085|     61|     True|              0.139|             0.356|\n",
            "|         7|Leonel|   52185|     70|    False|                1.0|             0.556|\n",
            "+----------+------+--------+-------+---------+-------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos observar que el ingreso mínimos es de 12560 y el máximo 52185."
      ],
      "metadata": {
        "id": "dp-mFhVg0EIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Normalizacion**\n",
        "Podemos normalizar nuestro dataset:"
      ],
      "metadata": {
        "id": "yenjoyoVQjuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Nomazlizar elementos aleatorios \n",
        "def normalizar(x):\n",
        "    #Vector para normalizar\n",
        "    listaCuadrada=x.map(lambda xi:xi*xi)\n",
        "    total=listaCuadrada.sum()\n",
        "    val=math.sqrt(total)\n",
        "    #Escalonar vector\n",
        "    lista= x.map(lambda xi :(xi/val))\n",
        "    return lista \n",
        "\n",
        "#Creamos datos\n",
        "df = spark.createDataFrame([ (1, 'Messi',12560,45,\"True\"),\n",
        "                             (2, 'James',42560,90,\"False\"),\n",
        "                             (3, 'Oliver',31285,81,\"True\"),\n",
        "                             (4, 'Isaac',25285,50,\"True\"),\n",
        "                             (5, 'Irving',32285,55,\"False\"),\n",
        "                             (6, 'Franco',18085,61,\"True\"),\n",
        "                             (7, 'Leonel',52185,70,\"False\"),\n",
        "                           ], [\"id_usuario\", \"Nombre\",\"Ingresos\",\"Por_dia\",\"Morocidad\"])\n",
        "\n",
        "#Mostrar datos iniciales\n",
        "print(\"Data :\")\n",
        "df.show(7)\n",
        "#############################################################\n",
        "lista = df.select('Ingresos').rdd.map(lambda row : row[0]).collect()\n",
        "Vector = sc.parallelize(lista,4)\n",
        "VectorEscalonada=normalizar(Vector)\n",
        "a=VectorEscalonada.collect()\n",
        "df2 = spark.createDataFrame(a, FloatType())\n",
        "df2=df2.withColumnRenamed(\"value\", \"Ingresos_Normalizado\")\n",
        "print(\"Columna Ingresos Normalizado :\")\n",
        "#Mostrar la columna\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_96ytzANQlO9",
        "outputId": "fe304e38-08ae-4614-ebc3-d7147c2a7794"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data :\n",
            "+----------+------+--------+-------+---------+\n",
            "|id_usuario|Nombre|Ingresos|Por_dia|Morocidad|\n",
            "+----------+------+--------+-------+---------+\n",
            "|         1| Messi|   12560|     45|     True|\n",
            "|         2| James|   42560|     90|    False|\n",
            "|         3|Oliver|   31285|     81|     True|\n",
            "|         4| Isaac|   25285|     50|     True|\n",
            "|         5|Irving|   32285|     55|    False|\n",
            "|         6|Franco|   18085|     61|     True|\n",
            "|         7|Leonel|   52185|     70|    False|\n",
            "+----------+------+--------+-------+---------+\n",
            "\n",
            "Columna Ingresos Normalizado :\n",
            "+--------------------+\n",
            "|Ingresos_Normalizado|\n",
            "+--------------------+\n",
            "|          0.14332211|\n",
            "|            0.485652|\n",
            "|          0.35699302|\n",
            "|          0.28852704|\n",
            "|          0.36840403|\n",
            "|          0.20636787|\n",
            "|           0.5954828|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Binarizacion**\n",
        "\n",
        "Consiste en transformar las cadenas de texto que tienen valores de string (True y False) a valores numericos, 0 para False y 1 para True"
      ],
      "metadata": {
        "id": "EXqt6g8wQpBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creamos los datos\n",
        "df = spark.createDataFrame([ (1, 'Messi',12560,45,\"True\"),\n",
        "                             (2, 'James',42560,90,\"False\"),\n",
        "                             (3, 'Oliver',31285,81,\"True\"),\n",
        "                             (4, 'Isaac',25285,50,\"True\"),\n",
        "                             (5, 'Irving',32285,55,\"False\"),\n",
        "                             (6, 'Franco',18085,61,\"True\"),\n",
        "                             (7, 'Leonel',52185,70,\"False\"),\n",
        "                           ], [\"id_usuario\", \"Nombre\",\"Ingresos\",\"Por_dia\",\"Morocidad\"])\n",
        "\n",
        "#Mostramos los datos\n",
        "print(\"Data :\")\n",
        "df.show()\n",
        "\n",
        "#Aqui podemos elegir en que columnas aplicar la binarizacion\n",
        "cols = ['Morocidad']\n",
        "\n",
        "#Aplicar la binarizacion al dataframe spark\n",
        "df_reduced = reduce(lambda df, c: df.withColumn(c, F.when(df[c] == 'False', 0.0).otherwise(1.0)), cols, df)\n",
        "\n",
        "#Mostrar columna binarizada\n",
        "print(\"Data + Columna binarizada:\")\n",
        "df_reduced.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUaf0DnSQrug",
        "outputId": "07632947-2960-4eae-f5e8-34d37e6902bf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data :\n",
            "+----------+------+--------+-------+---------+\n",
            "|id_usuario|Nombre|Ingresos|Por_dia|Morocidad|\n",
            "+----------+------+--------+-------+---------+\n",
            "|         1| Messi|   12560|     45|     True|\n",
            "|         2| James|   42560|     90|    False|\n",
            "|         3|Oliver|   31285|     81|     True|\n",
            "|         4| Isaac|   25285|     50|     True|\n",
            "|         5|Irving|   32285|     55|    False|\n",
            "|         6|Franco|   18085|     61|     True|\n",
            "|         7|Leonel|   52185|     70|    False|\n",
            "+----------+------+--------+-------+---------+\n",
            "\n",
            "Data + Columna binarizada:\n",
            "+----------+------+--------+-------+---------+\n",
            "|id_usuario|Nombre|Ingresos|Por_dia|Morocidad|\n",
            "+----------+------+--------+-------+---------+\n",
            "|         1| Messi|   12560|     45|      1.0|\n",
            "|         2| James|   42560|     90|      0.0|\n",
            "|         3|Oliver|   31285|     81|      1.0|\n",
            "|         4| Isaac|   25285|     50|      1.0|\n",
            "|         5|Irving|   32285|     55|      0.0|\n",
            "|         6|Franco|   18085|     61|      1.0|\n",
            "|         7|Leonel|   52185|     70|      0.0|\n",
            "+----------+------+--------+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Distancia de Minkowski**"
      ],
      "metadata": {
        "id": "sGgGvv_nQu6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creemos una función rNorm que toma como parámetro y devuelve una función que calcula el pNorm\n",
        "def pNorm(p):\n",
        "    def Dist(x,y):\n",
        "        return np.power(np.power(np.abs(x-y),p).sum(),1/float(p))\n",
        "    return Dist\n",
        "# Creemos un RDD con valores numéricos.\n",
        "np.random.seed(50)\n",
        "num_p = sc.parallelize(enumerate(np.random.random(size=(10,100))))"
      ],
      "metadata": {
        "id": "OK8w1aaGQxMv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_p"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To83Y5xCQykg",
        "outputId": "8f7b3512-97d0-4b24-95ea-63aadab016ae"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[148] at readRDDFromFile at PythonRDD.scala:262"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Formula Minswoski\n",
        "dat_p = num_p.cartesian(num_p)\n",
        "dato_p = dat_p.map(lambda x: ((x[0][0],x[1][0]), (x[0][1],x[1][1])))\n",
        "#minwoski el valor p tendra diferentes valores \n",
        "p = 5\n",
        "#p = 6\n",
        "# p = 7\n",
        "Minkow = pNorm(p)\n",
        "dist = dato_p.map(lambda x: (x[0], Minkow(x[1][0],x[1][1])))\n",
        "soluc = dist.map(lambda x: x[1])\n",
        "minv, maxv, meanv = soluc.min(), soluc.max(), soluc.mean()\n",
        "print('minimo valor minkowski: ',minv)\n",
        "print('maximo valor minkowski: ',maxv)\n",
        "print('Media de los valores minkowski: ',meanv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzzwYQg3Q1Ux",
        "outputId": "0714510f-892f-4c18-a9ac-e7cbf637cfcf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "minimo valor minkowski:  0.0\n",
            "maximo valor minkowski:  1.444514010466728\n",
            "Media de los valores minkowski:  1.2070128755149965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Distancia Jaccard**"
      ],
      "metadata": {
        "id": "7rUI9HP8Q3I4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calcula la distancia Jaccard entre dos vectores binarios.\n",
        "\n",
        "def Jaccard(x,y):\n",
        "    return (x==y).sum()/float( np.maximum(x,y).sum())\n",
        "\n",
        "colores = sc.parallelize(enumerate([['Amarillo', 'Rojo', 'Negro'],\n",
        "                             ['Rojo', 'Celeste', 'Amarillo'],\n",
        "                             ['Negro', 'Celeste', 'azul'],\n",
        "                             ['Amarillo', 'Rojo', 'Negro'],\n",
        "                             ['Negro', 'Amarillo', 'Celeste'],\n",
        "                            ]))\n",
        "datos = (colores\n",
        "             .flatMap(lambda x: [((x[0],xi),1) for xi in x[1]])\n",
        "             .reduceByKey(lambda x,y: x)\n",
        "             .map(lambda x: x[0])\n",
        "             )\n",
        "\n",
        "dato = dict((v,k) for k,v in datos.collect())\n",
        "ndato = len(dato)\n",
        "print(dato, ndato)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY8O2oPJQ7ky",
        "outputId": "9ef9c587-02df-4e41-b943-e442b1661865"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Amarillo': 3, 'Rojo': 0, 'Celeste': 1, 'Negro': 4, 'azul': 2} 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Binarizar el vector categórico usando un diccionario de key\n",
        "#atributos (lista): Lista de atributos de un objeto dado\n",
        "def Binarizar(atributos,dato):  \n",
        "    array = np.zeros(len(dato))\n",
        "    for atr in atributos:\n",
        "        array[ dato[atr] ] = 1\n",
        "    return array\n",
        "\n",
        "# Convierta datosa formato binario usando key  dict\n",
        "binarizar = colores.map(lambda rec: (rec[0],Binarizar(rec[1], dato)))\n",
        "binarizar.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdF0KqR4Q80Z",
        "outputId": "f2272871-764e-48fa-8d35-482cd056b1cc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, array([1., 0., 0., 1., 1.])),\n",
              " (1, array([1., 1., 0., 1., 0.])),\n",
              " (2, array([0., 1., 1., 0., 1.])),\n",
              " (3, array([1., 0., 0., 1., 1.])),\n",
              " (4, array([0., 1., 0., 1., 1.]))]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adquirir dentro de los  PySpark para hallar produto cartesiano \n",
        "Binario = binarizar.cartesian(binarizar)\n",
        "# Aplicar un mapa para transformar nuestro RDD en un RDD de tupla ((id1, id2), (vector1, vector2))\n",
        "# use el comando take (1) e imprima el resultado para verificar el formato RDD actual\n",
        "Binario_par = Binario.map(lambda x: ((x[0][0],x[1][0]), (x[0][1],x[1][1])))\n",
        "#Aplicar un mapa para calcular la distancia de Jaccard entre pares\n",
        "jacRDD = Binario_par.map(lambda x: (x[0], Jaccard(x[1][0],x[1][1])))\n",
        "#calcular min, max, mean\n",
        "statJRDD = jacRDD.map(lambda x: x[1])\n",
        "Jmin, Jmax, Jmean = statJRDD.min(), statJRDD.max(), statJRDD.mean()\n",
        "print (\"\\t\\tMin\\tMax\\tMedia\")\n",
        "print (\"Jaccard:\\t{:.2f}\\t{:.2f}\\t{:.2f}\".format( Jmin, Jmax, Jmean ))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHs0znJAQ-s3",
        "outputId": "47e68db5-d4ab-4481-ff31-837113ed9388"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\tMin\tMax\tMedia\n",
            "Jaccard:\t0.20\t1.67\t0.87\n"
          ]
        }
      ]
    }
  ]
}